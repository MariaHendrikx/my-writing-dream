# Personalized text-to-image Models -- Day 3

Alright. It's day 3. Today will be a very short blog, since I am starting at 23h in the evening... and it's a saturday. Today I will just continue the overview.

## Overview Research

Might be interesting to use:
- [StoryGen -- Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models](https://haoningwu3639.github.io/StoryGen_Webpage/)
    - March 2024
    - Was mentioned in Seed-Story
    - Code is available
- 
# Terminology

- The Vision Transformer (ViT) is a neural network architecture that applies the transformer model, originally designed for natural language processing, to image classification tasks by treating image patches as input tokens. Unlike traditional convolutional neural networks (CNNs), ViT leverages self-attention mechanisms to capture long-range dependencies and global context within an image, leading to state-of-the-art performance on various vision benchmarks. It significantly reduces inductive biases related to locality and translation invariance, typically present in CNNs.